{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Personalities of Programmers of Different Programming Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code that allows me to get the avreage personality of programmers of different programming langauges. My intuition is, that different programs attract different types of programmers. So here is a way to sort of test it. \n",
    "\n",
    "The data used here is the one found on Kaggle: [https://www.kaggle.com/stackoverflow/stacksample/version/1](https://www.kaggle.com/stackoverflow/stacksample/version/1). If you want to replicate the analysis, please use the data there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I need to import all the libaries that I am going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson import PersonalityInsightsV3\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "import requests\n",
    "import pandas\n",
    "import json\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, I am setting come constants, that I am going to be using below. In order to get the personalities, I use the IBM's personality insights ([https://www.ibm.com/watson/services/personality-insights/](https://www.ibm.com/watson/services/personality-insights/)). If you want to replicate it, you need to get the keys from IBM. Up to 1000 calls/month it is free (and this script uses less than 1000). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_personality = \"personality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "  \"apikey\": \"\",\n",
    "  \"iam_apikey_description\": \"\",\n",
    "  \"iam_apikey_name\": \"\",\n",
    "  \"iam_role_crn\": \"\",\n",
    "  \"iam_serviceid_crn\": \"\",\n",
    "  \"url\": \"https://gateway-fra.watsonplatform.net/personality-insights/api\"\n",
    "}\n",
    "api_link = \"https://gateway-fra.watsonplatform.net/personality-insights/api\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I need to get all the tags for each question. So I am parsing the file with tags and save all the tags in the list and creating dictionary of all the question with their tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tags = collections.defaultdict(list)\n",
    "all_tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Tags.csv\") as f:\n",
    "    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    for question, tag in reader:\n",
    "        if not question == \"Id\":\n",
    "            question_tags[question].append(tag)\n",
    "            all_tags.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not every tag is a programming langauge, I need to find a way to filter it. What I did was use StackOverflow 2019 survey to get the names of the most popular langauges. This is the script that allowed me to do this. I then save them in the langs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_lang = \"https://insights.stackoverflow.com/survey/2019#technology\"\n",
    "data = requests.get(url_lang)\n",
    "data_html = BeautifulSoup(data.text)\n",
    "data_html = data_html.find_all(id=\"technology-most-popular-technologies-all-respondents\")\n",
    "data_html = [l.lower() for l in data_html[0].get_text().split(\"\\n\") if l.strip() and not \"%\" in l][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = []\n",
    "for lang in data_html:\n",
    "    if \"/\" in lang:\n",
    "        lang = lang.split(\"/\")\n",
    "        langs += lang\n",
    "    else:\n",
    "        langs.append(lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am now going througj all the tags and filter first the tags and then the questions. I first filter the 1000 most popular tags on stackoverflow and then only keep the questions that have one of the tags that is both in 1000 most popular and in the language list. I do the same with answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_to_check = set([lang for lang, count in collections.Counter(all_tags).most_common(1000) if lang.lower() in langs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tags = dict([(q, list(set(tags).intersection(tags_to_check))) for q, tags in question_tags.items() if len(set(tags).intersection(tags_to_check)) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_questions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Questions.csv\", \"r\", -1, \"latin-1\") as read:\n",
    "    reader = csv.reader(read, delimiter=',', quotechar='\"')\n",
    "    for question, user, _, _, _, _, _ in reader:\n",
    "        if question in question_tags:\n",
    "            pairs_questions.append([question, user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_questions = dict(pairs_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Answers.csv\", \"r\", -1, \"latin-1\") as read:\n",
    "    reader = csv.reader(read, delimiter=',', quotechar='\"')\n",
    "    for _, user, _, question, _, _ in reader:\n",
    "        if question in question_tags:\n",
    "            pairs_answers.append([question, user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_answers = dict(pairs_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then combine the answers and question tags for each user. This way, I can then filter the users, so I am using only the users, that have at least half of the question and answers in one language. So there is limited cross-contamination of different persoanlties. I also filter users with less than 100 questions with these tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_tags = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, user in pairs_answers.items():\n",
    "    users_tags[user] += question_tags[question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, user in pairs_questions.items():\n",
    "    users_tags[user] += question_tags[question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_lang = []\n",
    "for user, tags in users_tags.items():\n",
    "    if len(tags) > 10:\n",
    "        most_common_tag_info = collections.Counter(tags).most_common(1)[0]\n",
    "        most_common_tag = most_common_tag_info[0]\n",
    "        tag_count = most_common_tag_info[1]\n",
    "        if tag_count > len(tags) * 0.5:\n",
    "            users_with_lang.append([user, most_common_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_lang = dict(users_with_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am checking, how many users have a certain langauge as their most popular one. I then filter by the most popular 15 langauges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_users_with_each_lang = collections.defaultdict(int)\n",
    "for user, lang in users_with_lang.items():\n",
    "    number_of_users_with_each_lang[lang] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_for_personality = set(pandas.DataFrame([(lang, count) for lang, count in number_of_users_with_each_lang.items()], columns=[\"Lang\", \"Count\"]).sort_values(by=\"Count\", ascending=False).head(15)[\"Lang\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I am collecting questions and answers for these users, that have one the the 15 langauges as their most popular one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_with_text = collections.defaultdict(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Questions.csv\", \"r\", -1, \"latin-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    for _, user, _, _, _, title, body in reader:\n",
    "        if user in users_with_lang:\n",
    "            if users_with_lang[user] in lang_for_personality:\n",
    "                user_with_text[user] += BeautifulSoup(title).get_text() + \" \"\n",
    "                user_with_text[user] += BeautifulSoup(body).get_text() + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Answers.csv\", \"r\", -1, \"latin-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    for _, user, _, _, _, body in reader:\n",
    "        if user in users_with_lang:\n",
    "            if users_with_lang[user] in lang_for_personality:\n",
    "                user_with_text[user] += BeautifulSoup(body).get_text() + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I am checking, how long is the 50th longest text for each langauges. This allows me to filter the 50 users for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_texts_per_lang = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, lang in users_with_lang.items():\n",
    "    if lang in lang_for_personality:\n",
    "        longest_texts_per_lang[lang].append(len(user_with_text[user]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_texts_per_lang = dict([(lang, sorted(counts)[-50:][0]) for lang, counts in longest_texts_per_lang.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c++': 165235,\n",
       " 'c#': 192436,\n",
       " 'ruby': 45695,\n",
       " 'java': 185852,\n",
       " 'c': 59555,\n",
       " 'javascript': 137794,\n",
       " 'sql': 116272,\n",
       " 'php': 101734,\n",
       " 'scala': 27057,\n",
       " 'python': 127637,\n",
       " 'objective-c': 79656,\n",
       " 'r': 54280,\n",
       " 'vba': 33050,\n",
       " 'swift': 30400,\n",
       " 'css': 28698}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_texts_per_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I am taking these texts, send them to the IBM's server, get back their personality and save it. You can find it in the folder personality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "personality_insights = PersonalityInsightsV3(version=\"2017-10-13\", iam_apikey=credentials[\"apikey\"], url=api_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, text in user_with_text.items():\n",
    "    lang = users_with_lang[user]\n",
    "    if lang in longest_texts_per_lang:\n",
    "        if len(text) >= longest_texts_per_lang[lang]:\n",
    "            filename = lang + \"_\" + user + \".json\"\n",
    "            profile = personality_insights.profile(content=text, accept=\"application/json\", content_language=\"en\", \n",
    "                                                   accept_language=\"en\", raw_scores=True, consumption_preferences=True,\n",
    "                                                   content_type=\"text/plain\"\n",
    "            ).get_result()\n",
    "            with open(os.path.join(folder_personality, filename), \"w\") as f:\n",
    "                f.write(json.dumps(profile))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
